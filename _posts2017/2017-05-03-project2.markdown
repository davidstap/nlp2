---
layout: post
title:  Project 2 
date:   2017-05-03
author: Wilker
categories: projects
---

[Project 2 description]({{ "/resources/project_crf/project2.pdf" | absolute_url }}).

*New deadline*: May 24 at 23:59 GMT-8.

`Note: The MBR part of the project is optional.`


# Code

* [roadmap](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/LV-CRF-Roadmap.ipynb): a tutorial style description of the project that connects theory and practice; in particular, we connect the concepts discussed in class with actual code; this tutorial uses `libitg` (see below);
* [libitg](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/libitg.py): is our implementation of Earley intersection between epsilon-free unweighted FSA and CFGs, we also provide functions to deal with ITGs and length constraints;
* [notebook on intersection](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/ITG.ipynb): you can use this notebook to understand more about the bits and pieces of the parser, but do not copy code from there, for that use [libitg](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/libitg.py)
* `new!` [notebook on faster parsing with finite Dx](https://github.com/uva-slpl/nlp2/blob/gh-pages/resources/notebooks/Fast-Parsing-with-Finite-Dx.ipynb)

# Reading

* Model
    * [Lecture slides]({{ "/resources/slides/crf.pdf" | absolute_url }})
    * [LV-CRF for Alignment](http://www.aclweb.org/anthology/P11-1042)
    * [LV-CRF for SMT](http://www.aclweb.org/anthology/P08-1024)
    * [Supervised CRFs](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)
    * [LV-CRF](https://www.cics.umass.edu/~mccallum/papers/entropygradient-naacl2007.pdf)
* Synchronous grammars
    * [ITGs](https://www.aclweb.org/anthology/J/J97/J97-3002.pdf)
    * [SCFG tutorial by David Chiang]({{ "/resources/papers/ChiangSCFG.pdf" | absolute_url }})
    * [Lecture slides]({{ "/resources/slides/itg.pdf" | absolute_url }})
* Bitext parsing
    * [Lecture slides]({{ "/resources/slides/bitext-parsing.pdf" | absolute_url }})
    * [Notes on Earley intersection]({{ "/resources/papers/Aziz-Earley.pdf" | absolute_url }})
    * [Principles and implementation of deductive parsing](https://arxiv.org/abs/cmp-lg/9404008)
    * [Cascade of parsers](http://www.aclweb.org/anthology/N10-1033)
* Hypergraphs
    * [Directed hypergraphs]({{ "/resources/papers/GalloEtAl.pdf" | absolute_url }})
    * [Parsing and hypergraphs](https://nlp.stanford.edu/manning/papers/klein_and_manning-parsing_and_hypergraphs-IWPT_2001.pdf)
* FSAs
    * [Knight and May's chapter on applications of weighted automata to NLP]({{ "/resources/papers/KnightAndMay.pdf" | absolute_url }})
    * [FSTs in language and speech processing](http://www.cs.nyu.edu/~mohri/pub/cl1.pdf)
    * [Mohri's chapter on weighted automata algorithms]({{ "/resources/papers/MohriWAA.pdf" | absolute_url }})
* Semirings 
    * [Goodman's semiring parsing artcile](http://www.aclweb.org/anthology/J/J99/J99-4004.pdf) 
    * [Expectation semiring for translation forests](http://www.aclweb.org/anthology/D09-1005)

# Tips

* To construct an ITG use the template in section 1 of the project description and the lexicon of translation pairs that we provided.
* ITGs can be rather large and the resulting forests may be quite nasty to deal with, thus we recommend you constrain your ITG so that it contains only top-scoring translation pairs (you can use the IBM1 probabilities that we provided, you can choose one direction or interpolate both). For development we recommend you work with at most 5 translations per source word, once everything is working, you can investigate the impact of having more options.
* Note that your ITG has to be able to insert and delete words, thus it has to have translation pairs involving the empty string, for that you can use alignments to NULL in the lexicon we provided.
* You can pre-compute parse forests and feature functions and save them to disk (for example using `cPickle`), that should save some time during iterations of SGD.


# Empty forests


Note that a proper ITG will always cover the training data. That's so because the lexicon would map every source type to every target type,  allow every source type to be deleted, and every target type to be inserted. Because we cannot afford optimising our implementations, we will be working with a constrained lexicon, this means that sometimes you will not be able to parse the source and/or the target observation. This may lead to an empty `D(x)` and/or an empty `D(x,y)` which makes the training instance useless.

The simplest strategy is to discard training instances for which D(x) and/or D(x,y) are empty, but that's a bit unsatisfactory and one may end up discarding too much data. Also, it is not general enough a solution and the problem will happen again with dev/test sets.

Two reasons why you get empty forests:
* your lexicon misses a certain translation pair 
* your lexicon cannot delete/insert a certain source/target word

## Unknown types

To deal with unknown types you can introduce an '-UNK-' type in your lexicon and map every word that is not in the lexicon to it. For example, you can have a rule `X -> '-UNK-'/'-UNK'` or even `X -> '-UNK-'/y` where y is a type in the target observation that is not in your lexicon (this last strategy has the added benefit that you can keep named-entities such as cities, organizations, or people's names). It is wise to have features for rules that deal with unknown types (e.g. an unknown indicator, a named-entity tag in case you have access to named entity recognisers, etc.).

## Deletion and Insertion

You need to be careful with deletion and insertion: it is not a good idea to constrain them. A strategy that will work for sure is to have all `X -> eps/y` and `X -> x/eps` possible. 

I wrote before that you could limit the insertions to the types in the target observation. I was probably sleepy. That's not at all an option! It's artificial to constrain the set of available insertions to the words in the target observation (because this is knowledge that is not available at prediction time).

You can (and you should), however, add deletion rules in case you are parsing a Chinese sentence that contains an unseen type. But note that if you mapped the unseen type to '-UNK-', then it's sufficient to add a rule `X -> '-UNK-'/'-EPS-'`.

Once more it's wise to have features for deletion/insertion of types that are unseen.

## Summary

There are may be several ways to combine the basic strategies and you may find your own, whatever you do, simply report what you did and try to justify it. 

I am going to document here one way to go about it.

* for each source type x, `X -> x/y` for the top 5 translations under `P(x|y) * P(y|x)` (with lexical parameters from IBM1)
* then every symbol that is not in the lexicon is mapped to '-UNK-' on both sides of the training data
* make sure unk maps to unk as well: `X -> '-UNK-'/'-UNK-'`
* I would also add the following deletion rules
    * `X -> '-UNK-'/'-EPS-'`
    * `X -> x/'-EPS-'` for every source type in the constrained lexicon
* and the following insertion rules
    * `X -> '-EPS-'/'-UNK-'`
    * `X -> '-EPS-'/y` for every target type in the constrained lexicon
* note that at prediction time (whenever you encounter a dev/test sentence) you first need to convert unseen source types into '-UNK-', then you can parse


# More nonterminals

Somebody asked me about having more nonterminals, I think the use case was

* `U -> x/eps` for some unseen type x
* and `U -> eps/y` for some unseen type y

this is possible too, but you would have to (at least) add a rule that upgrades `U` to `X`
* `X -> U`
but other strategies may be possible too.

Note that if you do this, you will need to revisit 3 functions:
* the function that makes the source side of an ITG assumed we were working with the basic rules only, so you need to account for your new rules there
* the function that projects a source forest onto the target vocabulary also assumed the basic rule set, so again, you need to modify it to include your additional rules
* the feature function prototype that I made available also makes a similar assumption

Crucially, note that the parser is left unchanged! That's because I've implemented a very general Earley intersection which can deal with any CFG.

In the spirit of playing with the rule set, note you can also have nonterminals that know more about deletion/insertion
* `I -> eps/y`
* `D -> x/eps`
* `T -> x/y` where x and y are not empty
and if you play with how I, D, and T interact you can further constrain the space of permutations. You can experiment with this if you like, but consider the changes carefully and justify your choices. 


# More features

Because you have access to the source sentence at all times, you can get rich source features of the source spans and edge covers. These features may help you get better segmentation and word order.

I call inside/outside span features those that summarise the context under a certain span and surrounding a certain span. For example, for an edge `X[2-6] -> X[2-3] X[3-6]` we know exactly which source phrase stands under 2-6, 2-3 and 3-6, in fact we also know which phrases stand in [bos-2] and [6-eos]. Phrases are very sparse though, thus they would make our feature vectors really huge! A good idea might be to use word embeddings to represent words (as a preprocessing step) and have an average embedding represent a phrase (in this case you would be adding as many features as dimensions in your embedding vector). You can also go beyond and use LSTM representations (if you know how to get them) which more naturally represent spans of text.

You can also use source features for reordering, for example, skip bigrams. A skip-bigram is a bigram which does not have to be adjacent, for example, in `le chien noir`, `le * chien`, `le * noir`, `chien * noir` are skip-bigrams. Because you always know the source span and the source string, you can always retrieve skip-bigrams. 

# Target language model features

Target language model features are really hard to use! I can help you with that in the next lecture if you are curious, but that's not going to be expected for project 2 as I would have to teach you a lot more about smart approximate intersection algorithms ;)
I am very familiar with those though, so if you are interested to go the extra mile, talk to me.
